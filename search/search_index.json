{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"From-Scratch Foundations","text":"<p>First-principles implementations, structured notes, and experiments for AI research \u2014 all built from scratch.</p> <ul> <li> <p> Math</p> <p>Linear algebra, probability, optimization, information theory.</p> <p> Browse notes</p> </li> <li> <p> Deep Learning</p> <p>Architectures, training dynamics, and from-scratch builds.</p> <p> Browse notes</p> </li> <li> <p> Reinforcement Learning</p> <p>MDPs, policy gradients, bandits, and planning.</p> <p> Browse notes</p> </li> <li> <p> NLP &amp; LLMs</p> <p>Transformers, tokenization, alignment, and evaluation.</p> <p> Browse notes</p> </li> <li> <p> DSA</p> <p>Data structures, algorithms, and complexity analysis.</p> <p> Browse notes</p> </li> </ul> <p> Quiet persistence brings clarity. </p>"},{"location":"nlp-llms/","title":"Index","text":""},{"location":"nlp-llms/#transformers","title":"Transformers","text":"<ul> <li>Transformers \u2014 Index</li> <li>Positional Encoding</li> </ul>"},{"location":"nlp-llms/transformers/","title":"Transformers","text":"<p>Short, atomic notes and implementation write-ups.</p>"},{"location":"nlp-llms/transformers/#position","title":"Position","text":"<ul> <li>Positional Encoding</li> <li>Sinusoidal vs Learned \u2014 Implementation Notes</li> <li>Learned Positional Embeddings</li> </ul>"},{"location":"nlp-llms/transformers/#building-blocks","title":"Building blocks","text":"<ul> <li>Embeddings Block (Token + Position + Dropout)</li> </ul>"},{"location":"nlp-llms/transformers/#engineering","title":"Engineering","text":"<ul> <li>Testing Numerics (<code>allclose</code>, tolerances)</li> </ul>"},{"location":"nlp-llms/transformers/embeddings-block/","title":"Embeddings Block (Token + Position + Dropout)","text":""},{"location":"nlp-llms/transformers/embeddings-block/#goal","title":"Goal","text":"<p>A single reusable module that converts token ids into model inputs.</p> <ul> <li>input: token ids <code>(B, L)</code></li> <li>output: embeddings <code>(B, L, D)</code></li> </ul>"},{"location":"nlp-llms/transformers/embeddings-block/#why-it-exists","title":"Why it exists","text":"<p>Centralizes: - <code>L &lt;= max_len</code> guard - positional choice (sinusoidal vs learned) - dropout application - predictable dtype/device behavior</p>"},{"location":"nlp-llms/transformers/embeddings-block/#interface","title":"Interface","text":"<p><code>forward(token_ids: LongTensor[B, L]) -&gt; FloatTensor[B, L, D]</code></p>"},{"location":"nlp-llms/transformers/embeddings-block/#tests-to-keep-forever","title":"Tests to keep forever","text":"<ul> <li>correct output shape</li> <li>raises on <code>L &gt; max_len</code></li> <li>deterministic in <code>.eval()</code> with dropout=0</li> </ul>"},{"location":"nlp-llms/transformers/learned-positional-embeddings/","title":"Learned Positional Embeddings","text":"<p>A learned positional embedding is a trainable lookup table mapping positions to vectors:</p> \\[ \\mathbf{p}_t = \\mathbf{E}[t], \\quad \\mathbf{E} \\in \\mathbb{R}^{\\text{max\\_len} \\times d_{\\text{model}}} \\]"},{"location":"nlp-llms/transformers/learned-positional-embeddings/#pros","title":"Pros","text":"<ul> <li>can adapt to the training distribution</li> <li>simple, fast, widely used</li> </ul>"},{"location":"nlp-llms/transformers/learned-positional-embeddings/#cons-constraints","title":"Cons / constraints","text":"<ul> <li>requires choosing <code>max_len</code></li> <li>extrapolation past training length is non-trivial (out-of-range positions)</li> </ul>"},{"location":"nlp-llms/transformers/learned-positional-embeddings/#engineering-rule","title":"Engineering rule","text":"<p>Fail fast when <code>seq_len &gt; max_len</code> unless you intentionally implement a long-context strategy.</p>"},{"location":"nlp-llms/transformers/positional-encoding-implementation/","title":"Positional Encoding \u2014 Implementation Notes","text":"<p>This documents the exact engineering contract used in the codebase.</p>"},{"location":"nlp-llms/transformers/positional-encoding-implementation/#contract","title":"Contract","text":"<ul> <li>Input: <code>x</code> with shape <code>(B, L, D)</code></li> <li>Output: tensor with shape <code>(B, L, D)</code></li> </ul>"},{"location":"nlp-llms/transformers/positional-encoding-implementation/#fail-fast-length-guard","title":"Fail-fast length guard","text":"<p>Enforce:</p> \\[ L \\le \\text{max\\_len} \\] <p>Raise <code>ValueError</code> if violated (clear error message beats silent slicing / cryptic index errors).</p>"},{"location":"nlp-llms/transformers/positional-encoding-implementation/#dtype-rule-mixed-precision","title":"Dtype rule (mixed precision)","text":"<p>Before adding PE to <code>x</code>, cast the PE slice to <code>x.dtype</code> and <code>x.device</code>:</p> <ul> <li>avoids silent fp16/bf16 \u2192 fp32 upcasting</li> <li>keeps performance predictable</li> <li>reduces test brittleness</li> </ul>"},{"location":"nlp-llms/transformers/positional-encoding-implementation/#dropout-testing","title":"Dropout + testing","text":"<p>To make tests stable: - use <code>dropout=0.0</code>, or - call <code>.eval()</code> during assertions</p>"},{"location":"nlp-llms/transformers/positional-encoding-implementation/#minimal-test-checklist","title":"Minimal test checklist","text":"<ul> <li>shape preserved</li> <li>raises on <code>L &gt; max_len</code></li> <li>dtype preserved</li> <li>determinism in <code>.eval()</code> (same seed)</li> <li>learned module has trainable params (<code>requires_grad=True</code>)</li> </ul>"},{"location":"nlp-llms/transformers/positional-encoding/","title":"Positional Encoding","text":"<p>Self-attention alone does not encode token order. Positional encoding injects order information into token representations.</p> <p>A common absolute approach adds a position vector to the token embedding:</p> \\[ \\mathbf{x}_t = \\mathbf{e}(w_t) + \\mathbf{p}_t \\] <p>Where: - \\(w_t\\) is the token at position \\(t\\) - \\(\\mathbf{e}(w_t) \\in \\mathbb{R}^{d_{\\text{model}}}\\) is its token embedding - \\(\\mathbf{p}_t \\in \\mathbb{R}^{d_{\\text{model}}}\\) encodes position</p>"},{"location":"nlp-llms/transformers/positional-encoding/#two-common-absolute-methods","title":"Two common absolute methods","text":""},{"location":"nlp-llms/transformers/positional-encoding/#sinusoidal-fixed","title":"Sinusoidal (fixed)","text":"<ul> <li>Deterministic, parameter-free</li> <li>Often extrapolates better to longer lengths</li> <li>Encodes position with sin/cos waves at multiple frequencies</li> </ul>"},{"location":"nlp-llms/transformers/positional-encoding/#learned-trainable","title":"Learned (trainable)","text":"<ul> <li>Trainable embedding table</li> <li>Can fit training distribution well</li> <li>Limited by <code>max_len</code> unless explicitly designed otherwise</li> </ul>"},{"location":"nlp-llms/transformers/positional-encoding/#repo-contract-used-here","title":"Repo contract used here","text":"<p>All positional modules follow: - Input: \\(x \\in \\mathbb{R}^{B \\times L \\times D}\\) - Output: same shape after adding position</p> <p>See: Implementation Notes.</p>"},{"location":"nlp-llms/transformers/testing-numerics/","title":"Testing Numerics (<code>allclose</code>)","text":""},{"location":"nlp-llms/transformers/testing-numerics/#why-exact-equality-is-brittle","title":"Why exact equality is brittle","text":"<p>Floating-point computations can differ slightly due to: - rounding/representation limits - different computation order (non-associativity) - different kernels/hardware</p> <p>So exact equality (<code>==</code>) often fails for correct results.</p>"},{"location":"nlp-llms/transformers/testing-numerics/#what-allclose-checks","title":"What <code>allclose</code> checks","text":"<p>Elementwise:</p> \\[ |a-b| \\le \\text{atol} + \\text{rtol}\\cdot |b| \\] <ul> <li><code>atol</code>: absolute tolerance (important near zero)</li> <li><code>rtol</code>: relative tolerance (scales with magnitude)</li> </ul>"},{"location":"nlp-llms/transformers/testing-numerics/#rule-of-thumb","title":"Rule of thumb","text":"<ul> <li>Use exact equality for integers, shapes, masks, indices.</li> <li>Use <code>allclose</code> for <code>sin/cos</code>, matmuls, reductions, GPU ops.</li> </ul>"},{"location":"reinforcement-learning/","title":"Reinforcement Learning","text":"<p>Notes and implementations covering MDPs, dynamic programming, policy gradients, contextual bandits, and planning algorithms.</p>"},{"location":"reinforcement-learning/#notes","title":"Notes","text":"Topic Status MDP fundamentals Policy gradient methods Contextual bandits (VW) Monte Carlo tree search <p>Reading queue</p> <ul> <li>Sutton &amp; Barto Ch. 1\u20135</li> <li>Lattimore &amp; Szepesv\u00e1ri (Bandit Algorithms)</li> <li>Agarwal et al. (RL: Theory and Algorithms)</li> </ul>"}]}