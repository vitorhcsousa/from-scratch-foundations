{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"From-Scratch Foundations","text":"<p>First-principles implementations, structured notes, and experiments for AI research \u2014 all built from scratch.</p> <ul> <li> <p> Math</p> <p>Linear algebra, probability, optimization, information theory.</p> <p> Browse notes</p> </li> <li> <p> Deep Learning</p> <p>Architectures, training dynamics, and from-scratch builds.</p> <p> Browse notes</p> </li> <li> <p> Reinforcement Learning</p> <p>MDPs, policy gradients, bandits, and planning.</p> <p> Browse notes</p> </li> <li> <p> NLP &amp; LLMs</p> <p>Transformers, tokenization, alignment, and evaluation.</p> <p> Browse notes</p> </li> <li> <p> DSA</p> <p>Data structures, algorithms, and complexity analysis.</p> <p> Browse notes</p> </li> </ul> <p> Quiet persistence brings clarity. </p>"},{"location":"nlp-llms/","title":"Index","text":""},{"location":"nlp-llms/#transformers","title":"Transformers","text":"<ul> <li>Transformers \u2014 Index</li> <li>Positional Encoding</li> </ul>"},{"location":"nlp-llms/transformers/","title":"Transformers","text":"<p>Short, atomic notes and implementation write-ups.</p>"},{"location":"nlp-llms/transformers/#position","title":"Position","text":"<ul> <li>Positional Encoding</li> <li>Sinusoidal vs Learned \u2014 Implementation Notes</li> <li>Learned Positional Embeddings</li> </ul>"},{"location":"nlp-llms/transformers/#building-blocks","title":"Building blocks","text":"<ul> <li>Embeddings Block (Token + Position + Dropout)</li> </ul>"},{"location":"nlp-llms/transformers/#engineering","title":"Engineering","text":"<ul> <li>Testing Numerics (<code>allclose</code>, tolerances)</li> </ul>"},{"location":"nlp-llms/transformers/embeddings-block/","title":"Embeddings Block","text":"<p>One module that replaces the repeated \"token embed \u2192 pos encode \u2192 dropout\" boilerplate found in every Transformer variant.</p>"},{"location":"nlp-llms/transformers/embeddings-block/#why-this-abstraction-exists","title":"Why this abstraction exists","text":"<p>Every Transformer (encoder, decoder, encoder-decoder) starts the same way:</p> <ol> <li>Look up token embeddings \u2192 <code>nn.Embedding(vocab_size, d_model)</code></li> <li>Add positional information \u2192 sinusoidal or learned</li> <li>Apply dropout</li> </ol> <p>Without a shared block you copy-paste these three lines into every model and inevitably drift (different dropout placement, forgetting the <code>padding_idx</code>, swapping pos strategies mid-experiment without updating both encoder and decoder, etc.).</p> <p><code>TokenAndPositionalEmbedding</code> encapsulates the three steps behind a single <code>forward(token_ids) \u2192 embeddings</code> call, so the rest of the model never thinks about embedding logistics.</p>"},{"location":"nlp-llms/transformers/embeddings-block/#contract","title":"Contract","text":"Input <code>(B, L)</code> \u2014 <code>torch.long</code> token ids Output <code>(B, L, d_model)</code> \u2014 <code>torch.float</code> embeddings Guard \\(L &gt; \\text{max\\_len}\\) \u2192 <code>ValueError</code>"},{"location":"nlp-llms/transformers/embeddings-block/#constructor","title":"Constructor","text":"<pre><code>TokenAndPositionalEmbedding(\n    vocab_size: int,\n    d_model: int,\n    *,\n    max_len: int,\n    pos_kind: Literal[\"sinusoidal\", \"learned\"],\n    dropout: float = 0.0,\n    pad_id: int | None = None,\n)\n</code></pre>"},{"location":"nlp-llms/transformers/embeddings-block/#pos_kind-strategies","title":"<code>pos_kind</code> strategies","text":"Value Class used Learnable? Extra params <code>\"sinusoidal\"</code> <code>SinusoidalPositionalEncoding</code> No 0 <code>\"learned\"</code> <code>LearnedPositionalEmbedding</code> Yes \\(\\text{max\\_len} \\times d_{\\text{model}}\\) <p>Both positional modules share the same <code>__init__</code> / <code>forward</code> signature, so the embedding block can instantiate either one with identical arguments \u2014 see [[positional-encoding-implementation]].</p>"},{"location":"nlp-llms/transformers/embeddings-block/#design-decisions","title":"Design decisions","text":"<ul> <li>Dropout lives here, not inside the positional module. The positional   modules are kept pure (they add the signal and nothing else). Dropout   is applied once after the sum <code>token + position</code>, matching the original   Transformer paper.</li> <li><code>pad_id</code> flows to <code>nn.Embedding(padding_idx=...)</code>, which zeros the   padding vector and excludes it from gradient updates automatically.</li> <li><code>_POS_REGISTRY</code> dict maps string keys to classes, making it trivial   to add a third strategy (e.g. <code>\"rotary\"</code>) later without touching the   constructor logic.</li> </ul>"},{"location":"nlp-llms/transformers/embeddings-block/#minimal-usage","title":"Minimal usage","text":"<pre><code>from foundations.projects.transformer.embeddings import TokenAndPositionalEmbedding\n\nemb = TokenAndPositionalEmbedding(\n    vocab_size=30_000,\n    d_model=512,\n    max_len=1024,\n    pos_kind=\"sinusoidal\",\n    dropout=0.1,\n    pad_id=0,\n)\n\nids = torch.randint(0, 30_000, (2, 128))   # (B, L)\nx = emb(ids)                                # (B, L, 512)\n</code></pre>"},{"location":"nlp-llms/transformers/embeddings-block/#links","title":"Links","text":"<ul> <li>[[positional-encoding]] \u2014 conceptual overview (why position matters)</li> <li>[[positional-encoding-implementation]] \u2014 the two positional modules this block depends on</li> <li>[[learned-positional-embeddings]] \u2014 deeper look at the trainable variant</li> <li>[[testing-numerics]] \u2014 why tests use <code>allclose</code> instead of <code>==</code></li> <li><code>src/foundations/projects/transformer/embeddings.py</code> \u2014 implementation</li> <li><code>tests/test_embeddings.py</code> \u2014 10 tests (shape, guards, determinism, padding)</li> </ul>"},{"location":"nlp-llms/transformers/learned-positional-embeddings/","title":"Learned Positional Embeddings","text":"<p>A trainable lookup table where each position gets its own independent vector \u2014 more expressive than sinusoidal, but capped at <code>max_len</code>.</p>"},{"location":"nlp-llms/transformers/learned-positional-embeddings/#why-a-learned-embedding","title":"Why a learned embedding","text":"<p>The sinusoidal scheme encodes position with a fixed formula that cannot adapt to the data. A learned embedding lets the model discover whatever positional patterns the training distribution rewards \u2014 relative spacing, boundary effects, attention sinks \u2014 all without hand-crafted frequencies.</p> <p>This is the approach used in BERT, GPT-2, and ViT.</p>"},{"location":"nlp-llms/transformers/learned-positional-embeddings/#math","title":"Math","text":"<p>Each position \\(t\\) maps to a row in a trainable matrix:</p> \\[ \\mathbf{p}_t = \\mathbf{E}[t], \\quad \\mathbf{E} \\in \\mathbb{R}^{\\text{max\\_len} \\times d_{\\text{model}}} \\] <p>The position vector is then added to the token embedding:</p> \\[ \\mathbf{x}_t = \\mathbf{e}(w_t) + \\mathbf{p}_t \\]"},{"location":"nlp-llms/transformers/learned-positional-embeddings/#trade-offs","title":"Trade-offs","text":"Sinusoidal (fixed) Learned Adapts to data No Yes Extra parameters 0 \\(\\text{max\\_len} \\times d_{\\text{model}}\\) Extrapolation Works for any position Undefined past <code>max_len</code> Simplicity Formula, no training needed Standard <code>nn.Embedding</code>"},{"location":"nlp-llms/transformers/learned-positional-embeddings/#design-decisions","title":"Design decisions","text":"<ul> <li>Fail fast on out-of-range positions. If \\(L &gt; \\text{max\\_len}\\) the   module raises <code>ValueError</code> immediately. Silently wrapping or clamping   indices would produce garbage embeddings that are hard to debug.</li> <li>Same interface as <code>SinusoidalPositionalEncoding</code>. Constructor takes   <code>(d_model, max_len, dropout)</code>, forward takes <code>(B, L, D)</code> and returns   <code>(B, L, D)</code>. This makes the two modules drop-in interchangeable \u2014 see   [[positional-encoding-implementation]].</li> <li>Long-context strategies are out of scope (for now). Techniques like   ALiBi, RoPE, or position interpolation solve the <code>max_len</code> ceiling but   belong in dedicated modules, not here.</li> </ul>"},{"location":"nlp-llms/transformers/learned-positional-embeddings/#minimal-usage","title":"Minimal usage","text":"<pre><code>from foundations.projects.transformer.positional_encoding import LearnedPositionalEmbedding\n\npos = LearnedPositionalEmbedding(d_model=512, max_len=1024, dropout=0.1)\nx = torch.randn(2, 128, 512)   # (B, L, D)\nout = pos(x)                     # (B, L, 512) \u2014 position added + dropout\n</code></pre>"},{"location":"nlp-llms/transformers/learned-positional-embeddings/#links","title":"Links","text":"<ul> <li>[[positional-encoding]] \u2014 conceptual overview (sinusoidal vs learned)</li> <li>[[positional-encoding-implementation]] \u2014 engineering contract both modules share</li> <li>[[embeddings-block]] \u2014 the combined token + position + dropout module</li> </ul>"},{"location":"nlp-llms/transformers/positional-encoding-implementation/","title":"Positional Encoding \u2014 Implementation","text":"<p>Two interchangeable <code>nn.Module</code> classes with identical signatures \u2014 swap sinusoidal for learned (or vice versa) with zero code changes.</p>"},{"location":"nlp-llms/transformers/positional-encoding-implementation/#why-two-modules-with-one-interface","title":"Why two modules with one interface","text":"<p>Experiments often compare fixed vs learned position signals. If the two implementations share the same <code>__init__</code> / <code>forward</code> contract, the rest of the model (and every test) works unchanged \u2014 only the constructor argument changes.</p>"},{"location":"nlp-llms/transformers/positional-encoding-implementation/#contract","title":"Contract","text":"<p>Both <code>SinusoidalPositionalEncoding</code> and <code>LearnedPositionalEmbedding</code> obey:</p> Input <code>(B, L, d_model)</code> \u2014 float embeddings Output <code>(B, L, d_model)</code> \u2014 same shape, position signal added Guard \\(L &gt; \\text{max\\_len}\\) \u2192 <code>ValueError</code>"},{"location":"nlp-llms/transformers/positional-encoding-implementation/#constructor","title":"Constructor","text":"<pre><code>SinusoidalPositionalEncoding(d_model: int, max_len: int = 5000, dropout: float = 0.1)\nLearnedPositionalEmbedding(d_model: int, max_len: int = 5000, dropout: float = 0.1)\n</code></pre> Class Learnable? Extra params Extrapolates? <code>SinusoidalPositionalEncoding</code> No 0 Yes <code>LearnedPositionalEmbedding</code> Yes \\(\\text{max\\_len} \\times d_{\\text{model}}\\) No"},{"location":"nlp-llms/transformers/positional-encoding-implementation/#design-decisions","title":"Design decisions","text":"<ul> <li>Fail-fast length guard. Both modules enforce \\(L \\le \\text{max\\_len}\\)   and raise <code>ValueError</code> if violated. A clear error beats silent slicing or   cryptic index-out-of-range crashes.</li> <li>Dtype / device cast before addition. The PE slice is cast to   <code>x.dtype</code> and <code>x.device</code> before adding, which avoids silent fp16/bf16 \u2192   fp32 upcasting, keeps performance predictable, and reduces test   brittleness.</li> <li><code>build_table</code> lives as a <code>@staticmethod</code>. The sinusoidal formula is   pure computation with no learnable state, so it sits inside   <code>SinusoidalPositionalEncoding</code> as a static method \u2014 callable without   instantiating the module, but scoped to the class that owns the logic.</li> <li>Buffer vs parameter. The sinusoidal table is registered via   <code>register_buffer</code> (travels with <code>.to(device)</code> / <code>.half()</code>, appears in   <code>state_dict</code>, but is never updated by the optimiser). The learned table   uses <code>nn.Embedding</code>, whose weights are parameters.</li> </ul>"},{"location":"nlp-llms/transformers/positional-encoding-implementation/#minimal-test-checklist","title":"Minimal test checklist","text":"<ul> <li>Shape preserved: <code>(B, L, D)</code> in \u2192 <code>(B, L, D)</code> out</li> <li>Raises on \\(L &gt; \\text{max\\_len}\\)</li> <li>Dtype preserved after addition</li> <li>Deterministic in <code>.eval()</code> mode (same input \u2192 same output)</li> <li>Sinusoidal: zero learnable params</li> <li>Learned: <code>requires_grad=True</code> on embedding weights</li> <li>See [[testing-numerics]] for why tests use <code>allclose</code></li> </ul>"},{"location":"nlp-llms/transformers/positional-encoding-implementation/#links","title":"Links","text":"<ul> <li>[[positional-encoding]] \u2014 conceptual overview (why position matters)</li> <li>[[learned-positional-embeddings]] \u2014 deeper look at the trainable variant</li> <li>[[embeddings-block]] \u2014 the combined token + position + dropout module</li> <li><code>src/foundations/projects/transformer/positional_encoding.py</code> \u2014 implementation</li> <li><code>tests/test_positional_encoding.py</code> \u2014 test suite</li> </ul>"},{"location":"nlp-llms/transformers/positional-encoding/","title":"Positional Encoding","text":"<p>Self-attention is permutation-invariant \u2014 without an explicit position signal, the model cannot distinguish \"the cat sat\" from \"sat the cat\".</p>"},{"location":"nlp-llms/transformers/positional-encoding/#why-positional-encoding-exists","title":"Why positional encoding exists","text":"<p>The dot-product attention mechanism treats its inputs as a set: shuffling the token order produces identical attention weights. Positional encoding breaks this symmetry by injecting order information into every token representation before the first attention layer.</p> <p>The standard absolute approach adds a position vector to the token embedding:</p> \\[ \\mathbf{x}_t = \\mathbf{e}(w_t) + \\mathbf{p}_t \\] Symbol Meaning \\(w_t\\) Token at position \\(t\\) \\(\\mathbf{e}(w_t) \\in \\mathbb{R}^{d_{\\text{model}}}\\) Token embedding \\(\\mathbf{p}_t \\in \\mathbb{R}^{d_{\\text{model}}}\\) Position vector"},{"location":"nlp-llms/transformers/positional-encoding/#two-common-absolute-methods","title":"Two common absolute methods","text":"Method Source Learnable? Extrapolates? Sinusoidal (fixed) Vaswani et al., 2017 No Yes \u2014 formula works for any position Learned embedding BERT / GPT-2 Yes No \u2014 capped at <code>max_len</code> <ul> <li>Sinusoidal encodes each position with sin/cos waves at geometrically   spaced frequencies. Deterministic, parameter-free, and often generalises   better to unseen lengths. See [[positional-encoding-implementation]].</li> <li>Learned uses a trainable <code>nn.Embedding</code> table. More expressive (each   position gets an independent vector) but limited by the pre-allocated   <code>max_len</code>. See [[learned-positional-embeddings]].</li> </ul>"},{"location":"nlp-llms/transformers/positional-encoding/#repo-contract","title":"Repo contract","text":"<p>All positional modules in this codebase follow the same interface:</p> Input <code>(B, L, d_model)</code> \u2014 float embeddings Output <code>(B, L, d_model)</code> \u2014 same shape after adding position Guard <code>L &gt; max_len</code> \u2192 <code>ValueError</code> <p>This makes them drop-in interchangeable inside [[embeddings-block|TokenAndPositionalEmbedding]].</p>"},{"location":"nlp-llms/transformers/positional-encoding/#links","title":"Links","text":"<ul> <li>[[positional-encoding-implementation]] \u2014 the two <code>nn.Module</code> classes and their engineering details</li> <li>[[learned-positional-embeddings]] \u2014 deeper look at the trainable variant</li> <li>[[embeddings-block]] \u2014 the combined token + position + dropout module</li> <li>[[testing-numerics]] \u2014 why tests use <code>allclose</code> instead of <code>==</code></li> </ul>"},{"location":"nlp-llms/transformers/testing-numerics/","title":"Testing Numerics (<code>allclose</code>)","text":"<p>Exact equality is the wrong tool for floats \u2014 use <code>torch.allclose</code> with explicit tolerances so tests stay green across hardware and dtypes.</p>"},{"location":"nlp-llms/transformers/testing-numerics/#why-exact-equality-breaks","title":"Why exact equality breaks","text":"<p>Floating-point arithmetic is not associative. Two mathematically identical computations can produce slightly different bits depending on evaluation order, fused-multiply-add availability, and GPU kernel selection. Testing with <code>==</code> rejects correct results over noise in the last few ULPs.</p>"},{"location":"nlp-llms/transformers/testing-numerics/#what-allclose-checks","title":"What <code>allclose</code> checks","text":"<p>Elementwise, the assertion passes when:</p> \\[ |a - b| \\le \\text{atol} + \\text{rtol} \\cdot |b| \\] Parameter Role Default (<code>torch</code>) <code>atol</code> Absolute tolerance \u2014 dominates near zero <code>1e-8</code> <code>rtol</code> Relative tolerance \u2014 scales with magnitude <code>1e-5</code>"},{"location":"nlp-llms/transformers/testing-numerics/#when-to-use-which","title":"When to use which","text":"Assertion Use for Exact <code>==</code> Integers, shapes, masks, indices, string/enum comparisons <code>allclose</code> Anything that touches floating-point math: sin/cos, matmuls, reductions, softmax, GPU ops"},{"location":"nlp-llms/transformers/testing-numerics/#design-decisions","title":"Design decisions","text":"<ul> <li>Always pass <code>atol</code> / <code>rtol</code> explicitly in new tests. Relying on   defaults works today but breaks silently when someone switches from   fp32 to bf16 (which needs looser tolerances).</li> <li>Keep a project-wide constant if tolerances recur. One <code>ATOL</code>, <code>RTOL</code>   pair in a <code>conftest.py</code> fixture avoids magic numbers scattered across   dozens of test files.</li> <li>Disable randomness before asserting. Set <code>dropout=0.0</code> or call   <code>.eval()</code> so the only variation left is floating-point noise, not   stochastic masking \u2014 see [[positional-encoding-implementation#minimal-test-checklist]].</li> </ul>"},{"location":"nlp-llms/transformers/testing-numerics/#minimal-usage","title":"Minimal usage","text":"<pre><code>import torch\n\na = torch.sin(torch.tensor(3.14159))\nb = torch.tensor(0.0)\n\n# default tolerances \u2014 fine for fp32\nassert torch.allclose(a, b, atol=1e-5)\n\n# tighter check when you know values are exact\npe1 = build_table(32, 64)\npe2 = build_table(32, 64)\nassert torch.allclose(pe1, pe2)          # deterministic \u2192 zero diff\n</code></pre>"},{"location":"nlp-llms/transformers/testing-numerics/#links","title":"Links","text":"<ul> <li>[[positional-encoding-implementation]] \u2014 test checklist that relies on <code>allclose</code></li> <li>[[embeddings-block]] \u2014 integration tests using <code>allclose</code> for shape + value checks</li> </ul>"},{"location":"reinforcement-learning/","title":"Reinforcement Learning","text":"<p>Notes and implementations covering MDPs, dynamic programming, policy gradients, contextual bandits, and planning algorithms.</p>"},{"location":"reinforcement-learning/#notes","title":"Notes","text":"Topic Status MDP fundamentals Policy gradient methods Contextual bandits (VW) Monte Carlo tree search <p>Reading queue</p> <ul> <li>Sutton &amp; Barto Ch. 1\u20135</li> <li>Lattimore &amp; Szepesv\u00e1ri (Bandit Algorithms)</li> <li>Agarwal et al. (RL: Theory and Algorithms)</li> </ul>"}]}